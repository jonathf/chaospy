\documentclass[handout]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, pdfpages, pdflscape, lscape, color, listings, hyperref, amssymb, graphicx,textcomp,varioref, afterpage, subcaption, float, bm, tikz}

\global
\newcommand{\Fig}[1]{Figure \ref{#1}}
\newcommand{\fig}[1]{figure \ref{#1}}
\newcommand{\tab}[1]{table \ref{#1}}
\newcommand{\eq}[1]{equation \ref{#1}}
\newcommand{\Eq}[1]{Equation \ref{#1}}
\newcommand{\alg}[1]{algorithm \ref{#1}}
\newcommand{\Alg}[1]{Algorithm \ref{#1}}
\newcommand{\chp}[1]{chapter  \ref{#1}}
\newcommand{\Chp}[1]{Chapter  \ref{#1}}
\newcommand{\e}[1]{\cdot 10^{#1}}
\newcommand{\h}{\hbar}
\newcommand{\der}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dder}[2]{\frac{\partial^2 #1}{\partial #2^2}}
\newcommand{\p}{\boldsymbol{P}}
\newcommand{\q}{\boldsymbol{q}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_{\!Q}}
\newcommand{\inner}[1]{\left\langle#1\right\rangle_{\!Q}}
\newcommand{\coef}[2]{\frac{\inner{#1,#2}}{\norm{#2}^2}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\E}[1]{\mbox{E}\!\left(#1\right)}
\newcommand{\Var}[1]{\mbox{Var}\!\left(#1\right)}
\newcommand{\Cov}[1]{\mbox{Cov}\!\left(#1\right)}

\newenvironment{test}[1]
{
 \usebackgroundtemplate{}
 \color{gray!30!black}
   \begin{tikzpicture}[remember picture, overlay]
     \node[anchor = center, opacity=.25] (image) at (current page.center) {\includegraphics[scale=0.25]{chaospy_logo.jpg}};
   \end{tikzpicture}
 \begin{frame}[fragile,environment=chaospy]

}
{
 \end{frame}
}


\newenvironment{chaospy}[1]
{\color{gray!30!black}
     \color{gray!30!black}
     \usebackgroundtemplate{
   \begin{tikzpicture}[remember picture, overlay]
     \node[anchor = center, opacity=.25] (image) at (current page.center) {\includegraphics[scale=0.25]{chaospy_logo.jpg}};
   \end{tikzpicture}}
     \begin{frame}[fragile,environment=chaospy]
    \frametitle{{#1}}}
{\end{frame}}


\definecolor{keywords}{RGB}{255,0,90}
\definecolor{comments}{RGB}{0,0,113}
\definecolor{red}{RGB}{160,0,0}
\definecolor{green}{RGB}{0,150,0}


 \lstset{
escapeinside={||},
basicstyle=\ttfamily\footnotesize, 
columns=fixed
}


\usetheme{kalkulo}

\graphicspath{{./figures/}}


\title{Polynomial chaos expansions part I: Method Introduction}
\author{Jonathan Feinberg and Simen Tenn√∏e}


\begin{document}


\begin{frame}
  \maketitle
\end{frame}

\begin{frame}[fragile]{Lectures will include many examples using the
    Chaospy software}
  \begin{center}
     \includegraphics[width=.5\textwidth]{chaospy_logo.jpg}
  \end{center}
    \begin{alert}{A very basic introduction to scientific Python programming:}
    \scriptsize
      \href{http://hplgit.github.io/bumpy/doc/pub/sphinx-basics/index.html}{http://hplgit.github.io/bumpy/doc/pub/sphinx-basics/index.html}\\
%\verb;http://hplgit.github.io/bumpy/doc/pub/sphinx-basics/index.html;
  \end{alert}
  \begin{alert}{Installation instructions:}\\
  \scriptsize
      \href{https://github.com/hplgit/chaospy}{https://github.com/hplgit/chaospy}\\
%\verb;http://github.com/hplgit/chaospy/;
  \end{alert}
%     \begin{alert}{Interactive session:}\\
%   \scriptsize
% \href{http://10.50.3.247:8888/}{http://10.50.3.247:8888/}
% 
% %\verb;http://10.50.3.247:8888/;
%   \end{alert}
\end{frame}

\begin{frame}[fragile]{Example: bloodflow simulations}{}
    \begin{columns}
        \column{.5\textwidth}
        \begin{center}
    \includegraphics[width=.6\textwidth]{ntnu/arterialHumanPicture.png}
        \end{center}
        \column{.5\textwidth}
        \begin{center}
    \includegraphics[width=.6\textwidth]{ntnu/arterialTreeMascot3D.png}
        \end{center}
    \end{columns}
    \small
    \begin{flushright}
        In colaboration with V. Eck and L. Hellevik
    \end{flushright}
\end{frame}

\begin{frame}{Modelling require uncertainty quantification}{}
    \begin{columns}
        \column{.3\textwidth}
        \includegraphics[width=\textwidth]{ntnu/ID-10015904.jpg}
        \column{.3\textwidth}
        \includegraphics[width=\textwidth]{ntnu/STARFiSh-Logo_small_transparent.png}
        \column{.3\textwidth}
        \includegraphics[width=\textwidth]{chaospy_logo.jpg}
    \end{columns} \pause
    \begin{center}
    \includegraphics[width=.1\textwidth]{figures/south.pdf}
    \end{center}
    \begin{columns}
        \column{.5\textwidth}
        \includegraphics[width=\textwidth]{ntnu/AorticPressure_parameterUncertainty.png}
        \column{.5\textwidth}
        \includegraphics[width=\textwidth]{ntnu/results/sensitivity-pointOfinflection.png}
    \end{columns}
    \end{frame}



\begin{frame}[fragile]
  {Introducing a testcase as a working example}
  \pause
  \begin{align*}
    \frac{d u(x)}{dx} & =-au(x) & u(0) &= I
  \end{align*}
  \begin{itemize}
    \item[$u$] The quantity of interest
    \item[$x$] Spatial location
    \item[$a,I$] Parameters containting uncertainties
  \end{itemize}
  \pause
  \begin{center}
    \includegraphics[width=.5\textwidth]{probspace.png}
  \end{center}
\end{frame}



%  \begin{frame}
%   \frametitle{Introducing the problem}
%   We start by looking at the 1D problem:
%   The area if interest is
%   \[x=[0,10]\]
%  %  We also add the complication
%  %   \[a = \begin{cases}
%  %         a & t<5\\
%  %         2a & t \geq 5
%  %        \end{cases}\]
%  \end{frame}

\begin{frame}
 {This model can be analysed analytically}
 \[
 u(x; a, I) = Ie^{-ax}
 \]
%  \[E(u) = \int_0^{10}\]
\pause
Initially assume model parameters:
\begin{align*}
a &\sim \text{Uniform(0, 0.1)} \sim f_a(a) & I&=1\hbox{ (known)}
\end{align*}
\pause
  \begin{align*}
      \E{u} &=  \int\limits_{-\infty}^{\infty} u(x;a)f_a(a)da=
            10\int_0^{0.1}e^{-ax}da
    = 10\frac{1-   e^{-0.1x}}{x} \\
    \onslide<4->{ \Var{u} & =\int\limits_{-\infty}^{\infty} (u(x;a)-
    \E{u})^2f_a(a)da =
   20\frac{1 - e^{-0.2ax}}{x} - \left(10\frac{1-e^{-0.1x}}{x}\right)^2}
 \end{align*}
\end{frame}

\begin{frame}[fragile]
{In general, models can be analysed using Monte Carlo integration}

    \begin{figure}
    \includegraphics[width=\textwidth]{MC.png}
  \end{figure}
\end{frame}

\begin{chaospy}{Monte Carlo with Chaospy}
    \scriptsize
\begin{lstlisting}[language=python]
import chaospy as cp
import numpy as np

def u(x, a):
  return np.exp(-a*x)

|\pause|
dist_a = cp.Uniform(0,0.1)

samples_a = dist_a.sample(size=1000)
|\pause|
x = np.linspace(0, 10, 100)
|\pause|
samples_u = [u(x, a) for a in samples_a]
|\pause|
E = np.mean(samples_u, 0)
Var = np.var(samples_u, 0)
\end{lstlisting}
\end{chaospy}

\begin{frame}
  \frametitle{Convergence of Monte Carlo is slow}
  \begin{align*}
      \varepsilon_E &= \int_0^{10}|\E{u} - \E{\hat{u}}|\,dx &
      \varepsilon_{Var} &= \int_0^{10}|\Var{u} - \Var{\hat{u}}|\,dx
  \end{align*}
  \begin{center}
    \includegraphics[width=0.75\textwidth]{MC_convergence_1D_1.png}
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Assumption: mapping from input $q$ to output $u$ is smooth}
  \begin{center}
   \includegraphics[width=0.8\textwidth]{mapping.png}
  \end{center}
\end{frame}

\begin{frame}[fragile]{Using Lagrange polynomials to approximate $u(q)$ ($N$-th degree polynomial interpolation)}{}
    \begin{align*}
        u(x;a) &\approx \hat u_M(x;a) =
        \sum_{n=0}^N c_n(x) P_n(a) & N&=M+1,
    \end{align*}
    where
    \begin{itemize}
        \item[$c_n$] are model evaluations $u(x, a_n)$
        \item[$P_n$] are Lagrange polynomials:
            \begin{align*}
                P_n(a) &= \prod_{\substack{m=0 \\ m\neq n}}^N \frac{a-a_n}{a_m-a_n}
            \end{align*}
        \item[$a_n$] are collocation nodes
    \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The interpolation has much better convergence properties than Monte Carlo!}
  % TODO
  % extend poly left
  % legend: Monte Carlo, Lagrange, Mean, Variance
  \begin{center}
    \includegraphics[width=0.8\textwidth]{MC_convergence_1D_2_short.png}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Oscillations in Lagrange polynomials (for large $N$) destroy the convergence}
  \begin{center}
    \includegraphics[width=0.8\textwidth]{MC_convergence_1D_2.png}
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Let us introduce a better polynomial approximation: Polynomial Chaos (PC) theory}
  \begin{align*}
      u(x;a) & &\approx && \hat u_M(x;a) && =
      && \sum_{n=0}^N && c_n(x)\quad && P_n(a),\quad && N &= M+1\\
      &&  &&  &&  &&  && \text{Coefficient} && \text{Polynomial}
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{PC employs inner product spaces weighted with the probability distribution}
    \begin{align*}
        \inner{u,v} &= \E{u\cdot v} \qquad
        \norm{u} = \sqrt{\inner{u,u}} \\
    \uncover<2->{&= \int f_Q(q)u(x,q)v(x,q)dq}
  \end{align*}
  where $Q$ is a random vector, i.e. $(a,I)$.
  \pause\pause\newline

  \begin{alert}
      {Orthogonality:}
      \[\inner{ P_n,P_m} =
  \begin{cases}
    \norm{P_n}^2 & n = m \\
    0 & n \neq m
  \end{cases}\]
  \end{alert}
\end{frame}

\begin{frame}{Coefficients are determined by least squares minimization}{}
    \begin{align*}
        \onslide<1->{ \min_{c_0,\ldots,c_N} || u- \hat u_M||_Q^2&\\
          \vdots\qquad&\\
        \inner{\sum_{n=0}^N c_n P_n,P_k}}
        \onslide<2->{\!\!&=
        \sum_{n=0}^N c_n \inner{P_n,P_k}}
        \onslide<3->{=
        c_k \inner{P_k,P_k}}
        & \onslide<2->{k&=0,\dots,N}
        \end{align*}
        \begin{align*}
        \onslide<5>{
        c_k &= \coef{u}{P_k} & \text{Fourier coefficients}
        }
    \end{align*}
\end{frame}

\begin{frame}{Least squares minimization implies minimization of variance}{}
    \begin{align*}
        (c_0,\dots,c_N)
        &= \argmin_{c_0,\dots,c_N} \norm{u-\hat u_M} \\\\
        \onslide<2->{
        &= \argmin_{c_0,\dots,c_N} \norm{u-\hat u_M}^2 \\\\
        }
        \onslide<3->{
        &= \argmin_{c_0,\dots,c_N} \E{(u-\hat u_M)^2} \\\\
        }
        \onslide<4->{
        &= \argmin_{c_0,\dots,c_N} \Var{u-\hat u_M} \\\\
        }
    \end{align*}
\end{frame}

\begin{frame}{The mean and variance have a simpler form}{}
    \pause
    \begin{alert}{Assumption:}
        $P_0 = 1$
    \end{alert}
    \begin{align*}
        \onslide<3->{\E{\hat u_M} &=
        \E{\sum_{n=0}^N c_n P_n}}
        &
        \onslide<7->{\Var{\hat u_M} &=
        \Var{\sum_{n=0}^N c_n P_n}}
        \\
        \onslide<4->{&=
        \sum_{n=0}^N c_n \E{P_n}}
        &
        \onslide<8->{&=
        \sum_{\substack{n=0\\m=0}}^N c_n c_m
        \left(\E{\!P_nP_m\!}\!-\!\E{\!P_n\!}\!\E{\!P_m\!}\right)}
        \\
        \onslide<5->{&=
        \sum_{n=0}^N c_n \inner{P_n, P_0}}
        &
        \onslide<9->{&=
        \sum_{\substack{n=0\\m=0}}^N c_nc_m\inner{P_n,P_m}-c_0^2}
        \\
        \onslide<6->{\E{\hat u_M}&=c_0}
        &
        \onslide<10->{\Var{\hat u_M}&=
        \sum_{n=1}^N c_n^2\norm{P_n}}
    \end{align*}
\end{frame}

\begin{frame}
 \frametitle{Construct an orthogonal polynomial expansion using
 Gram-Schmidt orthogonalization}
%  Assumption: We are in 1D and have $a +~ F_q$ and $I = 1$.
 \[v_0, v_1,...,v_N = 1, q,...,q^N\]
 \pause
 The Gram Schmidt method is

 \begin{align*}
    P_0 &= v_0\\
    \uncover<3-> {P_n &= v_n - \sum_{m=0}^{n-1} \frac{\inner{v_n,P_m}}{\norm{P_m}^2}}\\
    \uncover<4-> {&= v_n -
    \sum_{m=0}^{n-1}\frac{\E{v_nP_m}}{\E{P_m^2}}}
 \end{align*}

\end{frame}

\begin{chaospy}{Gram-Schmidt with chaospy}
\begin{lstlisting}[language=python]
M = 5; N = M - 1

dist_a = cp.Uniform(0, 0.1)
|\pause|
v = cp.basis(0, M, 1)
P = [v[0]]
|\pause|
for n in range(1, N):
    p = v[n]
    for m in range(0, n):
        p -= P[m]*cp.E(v[n]*P[m], dist_a)
                            /cp.E(P[m]**2, dist_a)
    P.append(p)
P = cp.Poly(P)
 \end{lstlisting}
\end{chaospy}


\begin{frame}
 \frametitle{Plot of all generated polynomials}
  \begin{figure}
    \includegraphics[width=0.85\textwidth]{gramschmidtpoly.png}
  \end{figure}
\end{frame}


\begin{frame}
 \frametitle{Most constructors of orthogonal polynomials are
 ill-conditioned}
  % TODO
  % high: formula for error
%   \[\varepsilon_E = \int_0^{10}|E(u) - E(\hat{u})|\,dx\]
     \begin{figure}
    \includegraphics[width=0.85\textwidth]{gramschmidterror1.png}
  \end{figure}

 \end{frame}


\begin{frame}
 {The only numerically stable method for
 calculating orthogonal polynomials is through the three-term discretized
 Stiltjes recursion}
 \pause
 Three terms recursion relation:
 \begin{align*}
     P_{n+1} &= (x-A_n) P_n - B_n P_{n-1} &
     P_{-1} &= 0 & P_0 &= 1,
 \end{align*}
    \pause
   where
   \begin{align*}
   A_n &= \frac{\langle qP_n,P_n\rangle_Q}{\norm{P_n}^2}
   &
  B_n &=
  \begin{cases}
  \frac{\norm{P_n}^2}{\norm{P_{n-1}}^2} & n > 0\\
  \norm{P_n}^2 & n = 0
  \end{cases}
   \end{align*}
  \end{frame}

\begin{frame}
 \frametitle{Discretized Stiltjes method is numerically stable}
    % TODO
    % high: error-formula
    % med: trunkation
%  \begin{align*}
%   \varepsilon_E &= \int_0^{10}|E(u) - E(\hat{u})|\,dx &
%   \end{align*}

%Change TTR to Discretized Stiltjes in the plot!

    \begin{figure}
    \includegraphics[width=0.85\textwidth]{gramschmidterror2.png}
  \end{figure}
 \end{frame}


\begin{frame}
 \frametitle{People have found analytical orthogonal polynomials for many common probability distributions}
 %The askey scheme gives a relation between a probability distribution and the corresponding orhogonal polynomial.
 \begin{center}
  \begin{tabular}{lll}
 Distribution & Polynomial & Support \\\hline
  Gaussian & Hermite & $(-\infty, \infty)$ \\
  Gamma & Laguerre & $[0,\infty]$ \\
  Beta & Jacobi & $[a,b]$ \\
  Uniform & Legendre & $[a,b]$ \\\hline
   \end{tabular}
\end{center}
 \end{frame}


\begin{chaospy}{
    Three terms recursion in Chaospy}
\begin{lstlisting}[language=python]
dist_a = cp.Normal()
P = cp.orth_ttr(3, dist_a)

print P
[1.0, q0, q0^2-1.0, q0^3-3.0q0]
\end{lstlisting}
\end{chaospy}


%  \begin{chaospy}{Finding the Fourier coefficients}
%   \begin{lstlisting}[language=python]
%  def u(x,a):
%    ax = np.outer(a,x)
%    return np.exp(-ax)
%  |\pause|
%  m = 2
%  a = cp.Uniform(0,0.1)
%  x = np.linspace(0, 10, 100)
%  |\pause|
%  P, norm = cp.orth_ttr(m, a, retall=True)|\pause|
%  nodes, weights = cp.generate_quadrature(m+1, a,
%                                          rule="G")|\pause|
%  solves = u(x,nodes[0])|\pause|
%  u_hat, c = cp.fit_quadrature(P, nodes, weights,
%                               solves,retall=True)
%  \end{lstlisting}
%
%  \end{chaospy}
\begin{frame}
{Repetition of the problem}
 \[
 u(x; a, I) = Ie^{-ax}
 \]
%  \[E(u) = \int_0^{10}\]
\pause
Initially assume model parameters:
\begin{align*}
a &\sim \text{Uniform(0, 0.1)} \sim f_a(a) & I&=1\hbox{ (known)}
\end{align*}
\pause
  \begin{align*}
      \E{u} &= 10\frac{1-   e^{-0.1x}}{x} &
     \Var{u} & = 20\frac{1 - e^{-0.2ax}}{x} - \left(10\frac{1-   e^{-0.1x}}{x}\right)^2
 \end{align*}


%Things have changed, but maybe skip formulas and focus on the epsilons!

 \pause
   \begin{align*}
  \varepsilon_E &= \int_0^{10}|E(u) - E(\hat{u})|\,dx &
  \varepsilon_{Var} &= \int_0^{10}|Var(u) - Var(\hat{u})|\,dx
  \end{align*}
\end{frame}

\begin{frame}
\frametitle{Convergence of orthogonal polynomial approximation}

%Fix legend: Polynomial to Lagrange
    % TODO
    % med: truncation
  \begin{figure}
    \includegraphics[width=0.8\textwidth]{MC_convergence_1D_3.png}
  \end{figure}
   \end{frame}


\begin{frame}
 \frametitle{Next step: Extend the theory to multiple dimensions}
 \pause
  \begin{align*}
  P_n &= P^{(1)}_n, ..., P^{(D)}_{n_D}
  & n\longleftrightarrow (n_1, ..., n_D)
  \end{align*}
  \pause
  \includegraphics[width=\textwidth]{multidim.png}

 % \begin{tabular}{l|cccccc}
 % ... &&&&&\\
  % $\mathbf{P}_{4}$& ...&&&& \\
 %   $\mathbf{P}_{3}$ &	$\mathbf{P}_{3}\mathbf{P}_{0}$ & ... && &\\
 %   $\mathbf{P}_{2}$ &	$\mathbf{P}_{2}\mathbf{P}_{0}$ & $\mathbf{P}_{2}\mathbf{P}_{1}$ & ... &&& \\
 %   $\mathbf{P}_{1}$ &	$\mathbf{P}_{1}\mathbf{P}_{0}$ &$\mathbf{P}_{1}\mathbf{P}_{1}$& $\mathbf{P}_{1}\mathbf{P}_{2}$ & ...&&\\
 %   $\mathbf{P}_{0}$ &	$\mathbf{P}_{0}\mathbf{P}_{0}$ &$\mathbf{P}_{0}\mathbf{P}_{1}$& $\mathbf{P}_{0}\mathbf{P}_{2}$ & $\mathbf{P}_{0}\mathbf{P}_{3}$ & ...\\ \hline
%		     &	$\mathbf{P}_{0}$	&$\mathbf{P}_{1}$& $\mathbf{P}_{2}$ & $\mathbf{P}_{3}$ & $\mathbf{P}_{4}$& ...
%  \end{tabular}
 \end{frame}


\begin{frame}
  \frametitle{We need a mapping from multiple indices to single index}
  \begin{columns}[c]
    \column{.5\textwidth}
    \begin{center}
      Multi-index\\
    \begin{tabular}{c}
    \\
      $\mathbf{P}_{00}$\\
    $\mathbf{P}_{10} \quad \mathbf{P}_{01}$\\
    $\mathbf{P}_{20} \quad \mathbf{P}_{11}\quad \mathbf{P}_{02}$\\
    $\mathbf{P}_{30} \quad \mathbf{P}_{21}\quad \mathbf{P}_{12}\quad ...$
  \end{tabular}
    \end{center}
\column{.5\textwidth}

\begin{center}
Single-index\\
\begin{tabular}{c}
\\
    $\mathbf{P}_{0}$\\
    $\mathbf{P}_{1} \quad \mathbf{P}_{2}$\\
    $\mathbf{P}_{3} \quad \mathbf{P}_{4}\quad \mathbf{P}_{5}$\\
    $\mathbf{P}_{6} \quad \mathbf{P}_{7}\quad \mathbf{P}_{8}\quad ...$
  \end{tabular}
\end{center}
  \end{columns}
    \pause
    \begin{align*}
        N =
        \begin{pmatrix}
            M+D \\ M
        \end{pmatrix}
    \end{align*}

\end{frame}


\begin{frame}
 \frametitle{Orthogonality for multivariate polynomials}
\begin{align*}
    \inner{ \bm P_n, \bm P_m} &= \E{P_{n_1}^{(1)}\cdots P_{n_D}^{(D)}\cdot
    P_{m_1}^{(1)}\cdots P_{m_D}^{(D)}}\\\\
    \onslide<2-> {&= \E{P_{n_1}^{(1)}\cdot P_{m_1}^{(1)}}\cdots
  \E{P_{n_D}^{(D)}\cdot P_{m_D}^{(D)}}} \\\\
  \onslide<3-> {&= \inner{ P_{n_1}^{(1)}, P_{m_1}^{(1)} }\cdots
  \inner{ P_{n_D}^{(D)}, P_{m_D}^{(D)} }} \\\\
  \onslide<4-> {&=
  \norm{P_{n_1}^{(1)}}\delta_{n_1m_1}\cdots\norm{P_{n_D}^{(D)}}\delta_{n_Dm_D}}
  \\\\
  \onslide<5-> {\inner{ \bm P_n, \bm P_m}&=
  \norm{\bm P_n} \delta_{nm}}
\end{align*}

\end{frame}


 \begin{chaospy}{Creating multivariate orthogonal polynomials in Chaospy}
\begin{lstlisting}[language=python]
dist_a = cp.Uniform(0, 0.1)
dist_I = cp.Uniform(8, 10)
dist = cp.J(dist_a, dist_I)
|\pause|
P = cp.orth_ttr(1, dist)
print P
[1.0, q1-9.0, q0]
|\pause|
P = cp.orth_ttr(3, dist)
print cp.E(P[1]*P[2],dist)
0.0 |\pause|
print cp.E(P[3]*P[3],dist)
0.0888888888903
\end{lstlisting}
\end{chaospy}


 \begin{frame}{A two-dimensional problem}
 \[  u(x; a, I) = Ie^{-ax}\]
\pause
Uncertain model parameters:
\begin{align*}
a &\sim \text{Uniform(0, 0.1)} & I&= \text{Uniform(8, 10)}
\end{align*}
 \pause
   \begin{align*}
  \varepsilon_E &= \int_0^{10}|E(u) - E(\hat{u})|\,dx &
  \varepsilon_{Var} &= \int_0^{10}|Var(u) - Var(\hat{u})|\,dx
  \end{align*}
\end{frame}


\begin{frame}
 \frametitle{Convergence of the two-dimensional $(a,I)$ problem}
 % TODO
 % high: error formulae
%   \begin{align*}
%   \varepsilon_E &= \int_0^{10}|E(u) - E(\hat{u})|\,dx &
%   \varepsilon_{Var} &= \int_0^{10}|Var(u) - Var(\hat{u})|\,dx
%   \end{align*}

\begin{figure}
    \includegraphics[width=0.85\textwidth]{MC_convergence_2D.png}
  \end{figure}

 \end{frame}


 \begin{chaospy}{Teaser of the full implementation}
     \scriptsize
 \begin{lstlisting}[language=python]
def u(x,a, I):
  return I*np.exp(-a*x)
|\pause|
dist_a = cp.Uniform(0, 0.1)
dist_I = cp.Uniform(8, 10)
dist = cp.J(a,I)|\pause|
|\pause|
P = cp.orth_ttr(2, dist)
|\pause|
nodes, weights = \
    cp.generate_quadrature(3, dist, rule="G")

x = np.linspace(0, 10, 100)
samples_u = [u(x, *node) for node in nodes.T]

u_hat = cp.fit_quadrature(P, nodes, weights, samples_u)

mean, var = cp.E(u_hat, dist), cp.Var(u_hat, dist)
 \end{lstlisting}
\end{chaospy}





\begin{frame}
  \frametitle{The curse of dimensionality}
  \includegraphics[height = 0.85\textheight]{dimensionality.png}

\end{frame}


\begin{frame}
  \frametitle{Gibb's Phenomena: discontinuities give oscillations}
  \begin{figure}
  \includegraphics[width=0.85\textwidth]{gibbs.png}
  \end{figure}
  \end{frame}

\begin{frame}
 \frametitle{Higher number of samples justifies higher number of
 collocation nodes}
\includegraphics[width=0.9\textwidth]{k_convergence.png}
 \end{frame}

 
 
\begin{frame}[fragile]{Thank you}
  \begin{center}
     \includegraphics[width=.5\textwidth]{chaospy_logo.jpg}
  \end{center}
    \begin{alert}{A very basic introduction to scientific Python programming:}
    \scriptsize
      \href{http://hplgit.github.io/bumpy/doc/pub/sphinx-basics/index.html}{http://hplgit.github.io/bumpy/doc/pub/sphinx-basics/index.html}\\
%\verb;http://hplgit.github.io/bumpy/doc/pub/sphinx-basics/index.html;
  \end{alert}
  \begin{alert}{Installation instructions:}\\
  \scriptsize
      \href{https://github.com/hplgit/chaospy}{https://github.com/hplgit/chaospy}\\
%\verb;http://github.com/hplgit/chaospy/;
  \end{alert}
%     \begin{alert}{Interactive session:}\\
%   \scriptsize
% \href{http://10.50.3.247:8888/}{http://10.50.3.247:8888/}
% 
% %\verb;http://10.50.3.247:8888/;
%   \end{alert}
\end{frame}
 

%\begin{frame}
%  \frametitle{What happens if we choose the wrong distribution}

%\end{frame}


\end{document}
